################ Importation des packages ################

library(ade4)
library(pls)
library(ClustOfVar)
library(glmnet)
library(Factoshiny)

################ Importation des données ################

genus <- read.csv("Datagenus.csv", sep = "\t", header = TRUE, dec = ",")
colnames(genus)

# Suppression de la variable "forest"
genus$forest <- NULL
colnames(genus)

############################ 1. Calculs de variables ###########################

# a) Calcul de la variable dépendante Y

Y <- rowSums(genus[,2:28])/genus$surface

# b) Calcul du tableau des variables explicatives #######

# Tableau d'indicatrices
geology <- acm.disjonctif(as.data.frame(genus$geology))
noms <- paste0("geology_", c(1,2,3,5,6))
colnames(geology) <- noms

# Tableau des variables quantitatives
geo_quant <- genus[c("lat", "lon", "altitude","pluvio_yr")]
for (i in 1:12) {
  geo_quant <- cbind(geo_quant, genus[paste0("pluvio_", i)])
}

# Tableau des variables EVI
EVI <- paste0("evi_", 1:23)
EVI <- genus[EVI]

# Tableau des variables explicatives 
X <- geo_quant
noms <- c()
for (i in colnames(geology)) {
  for (j in colnames(geo_quant)) {
    X <- cbind(X, geo_quant[j] * geology[i])
    noms <- cbind(noms, paste0(i , ".", j))
  }
}
colnames(X)[17:96] <- noms
X <- cbind(X, geology, EVI)

# Multicolinéarité :
# variables quantitatives avec les variables d'interactions

############## 2. Première régression sur composantes principales ##############

# a) ACP globale

ACP = PCA(X, ncp = 8)
# Factoshiny(ACP)

#Standardisation
variance <- function(x){sum((x-mean(x))^2)/length(x)}
standard <- function(x){(x-mean(x))/(sqrt(variance(x)))}

X <- apply(X,2,standard)

comp <- as.matrix(X) %*% ACP$svd$V

# On pourra retenir les 8 premières composantes

# b) Modélisation de la densité

model1 <- lm(Y ~ comp)
summary(model1)

# On enlève les composantes 7 et 8

model1 <- lm(Y ~ comp[, -c(7, 8)])
summary(model1)

# R² = 0.2105

plot(Y, model1$fitted.values)
abline(a = 0, b = 1)

# Modèle mauvais

# c) Coefficients des variables originelles

# Y_hat = C*beta (avec C les componsantes)
# Or C = X*V
# Donc Y_hat = X*V*beta => V*beta sont les coefficients des variables de X

coeff <- ACP$svd$V[, 1:6] %*% model1$coefficients[-1]
rownames(coeff) <- colnames(X)
coeff

# contrib_total <- rowSums(ACP$var$contrib[, 1:6])
# 
# contrib <- ACP$var$contrib[, 1:6]
# 
# coeff <- matrix(NA, nrow = 124, ncol = 6)
# for (i in 1:124){
#   coeff[i, ] <- contrib[i, ] * model1$coefficients[-1] / contrib_total[i]
# }
# 
# rownames(coeff) <- colnames(contrib)

# d) Correction log

zero <- which(Y == 0)

model1_bis <- lm(log(Y[-zero]) ~ comp[-zero,])
summary(model1_bis)

# On enlève la composante 7

model1_bis <- lm(log(Y[-zero]) ~ comp[-zero, -7])
summary(model1_bis)

# R² = 0.2673

plot(log(Y[-zero]), model1_bis$fitted.values)
abline(a = 0, b = 1)

coeff_bis <- ACP$svd$V[, -7] %*% model1_bis$coefficients[-1]
rownames(coeff_bis) <- colnames(X)
coeff_bis

############## 3. Seconde régression sur composantes principales ###############

geo <- X[, 1:101]

# ACP
ACP_geo <- PCA(geo)
ACP_evi <- PCA(EVI)

ACP_geo$eig # On garde les 6 premières composantes
ACP_geo <- PCA(geo, ncp = 6)

ACP_evi$eig # On garde les 3 premières composantes
ACP_evi <- PCA(EVI, ncp = 3)

# Réunion des composantes
comp_union <- cbind(as.matrix(geo) %*% ACP_geo$svd$V,
                    as.matrix(EVI) %*% ACP_evi$svd$V)
colnames(comp_union) <- paste0("geo_dim", 1:9)
colnames(comp_union)[7:9] <- paste0("evi_dim", 1:3)

# Modélisation
model2 <- lm(Y ~ comp_union)
summary(model2)

# On enlève les composantes 4, 6, 7 et 9 de l'ensemble

model2 <- lm(Y ~ comp_union[, -c(4, 6, 7, 9)])
summary(model2)

# R² = 0.2035 (moins bon que le R² du modèle 1)

plot(Y, model2$fitted.values)
abline(a = 0, b = 1)

# Le nuage de points n'est toujours pas très proche de la droite,
# les données ne sont pas très bien estimées

# Coefficients des variables originelles

# Ici Y_hat = [Comp_geo, Comp_evi] * beta
# Or Com_geo = Geo * V_geo et Comp_evi = EVI * V_evi
# Donc Y_hat = [Geo * V_geo * beta_geo, EVI * V_evi * beta_evi]

coeff_u <- rbind(ACP_geo$svd$V[, -c(4, 6)] %*% model2$coefficients[-c(1, 6)],
                 ACP_evi$svd$V[, 2] %*% as.matrix(model2$coefficients[6]))
rownames(coeff_u) <- colnames(X)
coeff_u

# Correction log

model2_bis <- lm(log(Y[-zero]) ~ comp_union[-zero,])
summary(model2_bis)

# On enlève la composante 1, 3 et 6 de Geo et 1 et 2 et 3 de EVI

model2_bis <- lm(log(Y[-zero]) ~ comp[-zero, -c(1,3,6, 7, 8, 9)])
summary(model2_bis)

# R² = 0.07934

plot(log(Y[-zero]), model2_bis$fitted.values)
abline(a = 0, b = 1)

coeff_u_bis <- ACP$svd$V[, c(2, 4, 5)] %*% model2_bis$coefficients[-1]
rownames(coeff_u_bis) <- colnames(X)
coeff_u_bis

########################## 5. Régressions pénalisées ###########################

# a) Régression Ridge

model4 <- glmnet(X, Y, alpha = 0, family = "gaussian")
model4

# Meilleur R² = 0,3464

model4$beta[,100]

plot(Y, model4$a0[1] + as.matrix(X) %*% model4$beta[,100],
     ylab = "Y estimé", main = "Y estimé en fonction de Y")
abline(a = 0, b = 1, col = "red")

# # Représentation graphique de la valeur des coefficients en fonction de lambda
# plot_glmnet(model4, label = 10)
# 
# 
# # On fixe lambda par validation croisée
# model4_bis <- train(X, Y, method = "glmnet", metric = "RMSE",
#         trControl = trainControl(method = "repeatedcv", number = 5,
#                                  repeats = 50),
#         tuneGrid = data.frame(alpha = 0, lambda = model4$lambda))
# 
# # Valeur optimale de lambda
# lambda_opt_ridge <- as.numeric(model4_bis$bestTune[2])
# model4_bis$results[model4_bis$results[, 2] == lambda_opt_ridge, ]
# 
# model4_ter <- glmnet(X, Y, alha = 0, lambda = lambda_opt_ridge, family = "gaussian")
# model4_ter$beta
# 
# plot_glmnet(model4, label = 10, s = lambda_opt_ridge)

# b) Régression Lasso

model5 <- glmnet(X, Y, alpha = 1, family = "gaussian")
model5

# Meilleur R² = 0,4672

wmodel5$beta[, 100]

plot(Y, model5$a0[1] + as.matrix(X) %*% model5$beta[,100],
     ylab = "Y estimé", main = "Y estimé en fonction de Y")
abline(a = 0, b = 1, col = "red")

# # Représentation graphique de la valeur des coefficients en fonction de lambda
# plot_glmnet(model5, label = 10)
# 
# # On fixe lambda par validation croisée
# model5_bis <- train(X, Y, method = "glmnet", metric = "RMSE",
#                     trControl = trainControl(method = "repeatedcv", number = 5,
#                                              repeats = 10),
#                     tuneGrid = data.frame(alpha = 1, lambda = model5$lambda))
# 
# # Valeur optimale de lambda
# lambda_opt_lasso <- as.numeric(model5_bis$bestTune[2])
# model5_bis$results[model5_bis$results[, 2] == lambda_opt_lasso, ]
# 
# model5_ter <- glmnet(X, Y, alha = 0, lambda = lambda_opt_lasso)
# model5_ter$beta
# 
# plot_glmnet(model5, label = 10, s = lambda_opt_lasso)

########################## 6. Synthèse & conclusions ###########################

Coeffs <- cbind(coeff, coeff_u,
                "Ridge" = model4$beta[,100],
                "LASSO" = model5$beta[,100])

R2_1 <- 0.2105
R2_2 <- 0.2035
R2_ridge <- model4$dev.ratio[100]
R2_lasso <- model5$dev.ratio[100]

R2 <- cbind(R2_1, R2_2, R2_ridge, R2_lasso)

MSE_1 <- mean((Y - model1$fitted.values)^2)
MSE_2 <- mean((Y - model2$fitted.values)^2)
MSE_ridge <- mean((Y - (model4$a0[100] + as.matrix(X) %*% model4$beta[,100]))^2)
MSE_lasso <- mean((Y - (model5$a0[100] + as.matrix(X) %*% model5$beta[,100]))^2)

MSE <- cbind(MSE_1, MSE_2, MSE_ridge, MSE_lasso)

resum <- rbind(Coeffs, R2, MSE)
rownames(resum)[c(125, 126)] <- c(expression(R^2), "MSE")
